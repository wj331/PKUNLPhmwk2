{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 任务描述"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这次的任务就是实现一个seq2seq 机器翻译模型。基本的RNN seq2seq 模型已经给我们实现好了。我们任务就是在这基本的RNN模型上边更加进步，我这次就是实现了一个GRU 的seq2seq 模型 并且也使用了课上介绍的Bahdanau Attention 来获得一个更高的Bleu score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 数据准备 （Data Preparation）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sacrebleu.metrics import BLEU\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from tqdm import tqdm\n",
    "import argparse, time\n",
    "import numpy as np\n",
    "\n",
    "from torch import Tensor\n",
    "import random\n",
    "\n",
    "\n",
    "# 加载数据 (to load the data)\n",
    "def load_data(num_train):\n",
    "    zh_sents = {}\n",
    "    en_sents = {}\n",
    "    for split in ['train', 'val', 'test']:\n",
    "        zh_sents[split] = []\n",
    "        en_sents[split] = []\n",
    "        with open(f\"./data/zh_en_{split}.txt\", encoding='utf-8') as f:\n",
    "            for line in f.readlines():\n",
    "                zh, en = line.strip().split(\"\\t\")\n",
    "                zh = zh.split()\n",
    "                en = en.split()\n",
    "                zh_sents[split].append(zh)\n",
    "                en_sents[split].append(en)\n",
    "    num_train = len(zh_sents['train']) if num_train==-1 else num_train\n",
    "    zh_sents['train'] = zh_sents['train'][:num_train]\n",
    "    en_sents['train'] = en_sents['train'][:num_train]\n",
    "    print(\"训练集 验证集 测试集大小分别为\", len(zh_sents['train']), len(zh_sents['val']), len(zh_sents['test']))\n",
    "    return zh_sents, en_sents\n",
    "\n",
    "# 构建词表 (to build the vocabulary, map word to index)\n",
    "class Vocab():\n",
    "    def __init__(self):\n",
    "        self.word2idx = {}\n",
    "        self.word2cnt = {}\n",
    "        self.idx2word = []\n",
    "        self.add_word(\"[BOS]\")\n",
    "        self.add_word(\"[EOS]\")\n",
    "        self.add_word(\"[UNK]\")\n",
    "        self.add_word(\"[PAD]\")\n",
    "    \n",
    "    def add_word(self, word):\n",
    "        \"\"\"\n",
    "        将单词word加入到词表中\n",
    "        \"\"\"\n",
    "        if word not in self.word2idx:\n",
    "            self.word2cnt[word] = 1\n",
    "            self.word2idx[word] = len(self.idx2word)\n",
    "            self.idx2word.append(word)\n",
    "        self.word2cnt[word] += 1\n",
    "    \n",
    "    def add_sent(self, sent):\n",
    "        \"\"\"\n",
    "        将句子sent中的每一个单词加入到词表中\n",
    "        sent是由单词构成的list\n",
    "        \"\"\"\n",
    "        for word in sent:\n",
    "            self.add_word(word)\n",
    "    \n",
    "    def index(self, word):\n",
    "        \"\"\"\n",
    "        若word在词表中则返回其下标，否则返回[UNK]对应序号\n",
    "        \"\"\"\n",
    "        #return index of the word else return index of unknown if word not in dictionary\n",
    "        return self.word2idx.get(word, self.word2idx[\"[UNK]\"])\n",
    "    \n",
    "    def encode(self, sent, max_len):\n",
    "        \"\"\"\n",
    "        在句子sent的首尾分别添加BOS和EOS之后编码为整数序列\n",
    "        \"\"\"\n",
    "        #returns the index of sentence in dictionary\n",
    "        encoded = [self.word2idx[\"[BOS]\"]] + [self.index(word) for word in sent][:max_len] + [self.word2idx[\"[EOS]\"]]\n",
    "        return encoded\n",
    "    \n",
    "    def decode(self, encoded, strip_bos_eos_pad=False):\n",
    "        \"\"\"\n",
    "        将整数序列解码为单词序列\n",
    "        \"\"\"\n",
    "        #returns the word of sentence in dictionary\n",
    "        return [self.idx2word[_] for _ in encoded if not strip_bos_eos_pad or self.idx2word[_] not in [\"[BOS]\", \"[EOS]\", \"[PAD]\"]]\n",
    "    \n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        返回词表大小\n",
    "        \"\"\"\n",
    "        return len(self.idx2word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 模型原理介绍"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "序列到序列模型（Sequence-to-Sequence, seq2seq）是一种广泛应用于自然语言处理任务（如机器翻译、文本摘要等）的深度学习架构。其核心由两个部分组成：编码器（Encoder）和解码器（Decoder）。\n",
    "\n",
    "编码器的作用是将输入序列（如句子）转化为一个固定长度的上下文向量（Context Vector），表示输入信息的抽象特征。该编码器通常由RNN（循环神经网络）、LSTM（长短期记忆网络）或GRU（门控循环单元）构成。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GRU Cell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "本任务是以GRU来创造encoder 的。GRU（门控循环单元）编码器的作用是将输入序列转换为一个固定长度的向量表示，通常称为上下文向量（Context Vector）。这个上下文向量是对整个输入序列的关键特征和依赖关系的压缩表示。GRU通过逐步处理序列数据并更新隐藏状态（Hidden State）来完成这一任务。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GRUCell(nn.Module):\n",
    "    def __init__(self, input_size: int, hidden_size: int):\n",
    "        super(GRUCell, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        #Gates: Update and Reset\n",
    "        self.weight_ih = nn.Linear(input_size, 2 * hidden_size)\n",
    "        self.weight_hh = nn.Linear(hidden_size, 2 * hidden_size)\n",
    "\n",
    "        #Candidate hidden state\n",
    "        self.weight_candidate = nn.Linear(input_size, hidden_size)\n",
    "        self.weight_hh_candidate = nn.Linear(hidden_size, hidden_size)\n",
    "\n",
    "    def forward(self, input, hx):\n",
    "        \"\"\"\n",
    "        input: (batch_size, input_size)\n",
    "        hx: (batch_size, hidden_size)\n",
    "        \"\"\"\n",
    "        # Compute gates\n",
    "        gates = self.weight_ih(input) + self.weight_hh(hx)\n",
    "        z_t, r_t = torch.split(torch.sigmoid(gates), self.hidden_size, dim=1)  # Split into update/reset gates\n",
    "        \n",
    "        # Compute candidate hidden state\n",
    "        candidate = torch.tanh(self.weight_candidate(input) + r_t * self.weight_hh_candidate(hx))\n",
    "        \n",
    "        # Update hidden state\n",
    "        hx = (1 - z_t) * hx + z_t * candidate\n",
    "        return hx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encoder (编码器)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在此使用编程过的GRU Cell，带入我们的Encoder, 会有这些的好处：\n",
    "\n",
    "缓解梯度消失问题：GRU通过门控机制动态控制隐藏状态的更新，使得梯度在反向传播时能够更好地传递。\n",
    "\n",
    "捕捉长短期依赖：GRU中的更新门能够决定保留多少历史信息和引入多少新信息，这种动态调整使得GRU能够同时记住短期和长期的依赖关系。\n",
    "\n",
    "提高训练稳定性：GRU的门控机制使得模型在训练时更稳定，收敛更快，尤其是在数据量有限的情况下。\n",
    "\n",
    "适应不同长度的输入序列：GRU能够根据输入序列长度动态调整记忆机制，从而在短序列和长序列任务中都能表现出色。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderRNN(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_size):\n",
    "        super(EncoderRNN, self).__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.hidden_size = hidden_size\n",
    "        self.gru = GRUCell(embedding_dim, hidden_size)\n",
    "        \n",
    "    \n",
    "    def forward(self, input, hidden):\n",
    "        \"\"\"\n",
    "        input: N\n",
    "        hidden: N * H\n",
    "        \n",
    "        输出更新后的隐状态hidden（大小为N * H）\n",
    "        \"\"\"\n",
    "        # seq_len, batch_size = input_seq.size()\n",
    "        # outputs = torch.zeros(seq_len, batch_size, self.hidden_size, device=input_seq.device)\n",
    "\n",
    "        # embedded = self.embedding(input)\n",
    "\n",
    "        # for t in range(seq_len):\n",
    "        #     hidden = self.gru(embedded[t], hidden)\n",
    "        #     outputs[t] = hidden\n",
    "        # return outputs, hidden\n",
    "        embedded = self.embedding(input)\n",
    "        hidden = self.gru(embedded, hidden)\n",
    "        return hidden"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attention Mechanism （注意力机制）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "解码器的作用是利用编码器生成的上下文向量，逐步生成输出序列。传统的seq2seq模型可能因为上下文向量过于简化而导致信息丢失。为了提升模型性能，我在解码器中引入了Bahdanau注意力机制。\n",
    "\n",
    "Bahdanau注意力机制通过动态地关注输入序列的不同部分，分配不同的权重，使得解码器在生成每个输出词时可以参考输入序列的相关部分。这样解决了固定长度上下文向量无法充分表达长序列信息的问题。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我会选择Bahdanau Attention的原因是因为它比传统注意力RNN更好 更合适：\n",
    "\n",
    "1. 更好地处理长序列\n",
    "对于长输入序列，传统注意力方法往往无法有效关注所有信息，因为权重分布可能失衡。\n",
    "Bahdanau注意力机制通过在每个解码步骤重新计算权重，确保即使是长距离依赖的信息也能被充分考虑。\n",
    "\n",
    "2. 解耦编码器与解码器上下文\n",
    "传统方法通常依赖一个固定的上下文向量（context vector）来概括整个输入序列，这种方法会丢失部分信息，尤其是长序列中的细节。\n",
    "Bahdanau注意力机制通过动态加权计算多个上下文向量，避免了固定上下文向量的局限性，从而保留了输入序列的更多信息"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BahdanauAttention(nn.Module):\n",
    "    def __init__(self, hidden_size):\n",
    "        super(BahdanauAttention, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.W = nn.Linear(hidden_size, hidden_size)\n",
    "        self.U = nn.Linear(hidden_size, hidden_size)\n",
    "        self.v = nn.Linear(hidden_size, 1)\n",
    "    \n",
    "    def forward(self, hidden, encoder_outputs):\n",
    "        \"\"\"\n",
    "        hidden: N * H\n",
    "        encoder_outputs: S * N * H\n",
    "        \n",
    "        输出attention加权后的context（大小为N * H）\n",
    "        \"\"\"\n",
    "        seq_len, batch_size, hidden_size = encoder_outputs.size()\n",
    "        hidden = hidden.unsqueeze(0).expand(seq_len, -1, -1)  # S * N * H\n",
    "        score = self.v(torch.tanh(self.W(hidden) + self.U(encoder_outputs)))  # S * N * 1\n",
    "        attention_weights = F.softmax(score, dim=0)  # S * N * 1\n",
    "        context = torch.sum(attention_weights * encoder_outputs, dim=0)  # N * H\n",
    "        return context, attention_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decoder （解码器）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在此把我们之前编码的Bahdanau 注意力机制带进我们的Decoder， 使得我们的解码器比传统解码器更加有优势比如：\n",
    "\n",
    "1. 动态关注：每一步解码时都能动态选择输入序列中的关键部分。\n",
    "2. 长序列处理能力更强：解决固定上下文向量导致的信息丢失问题。\n",
    "3. 梯度流动更好：训练更稳定，尤其在长序列任务中效果显著。\n",
    "4. 模型更可解释：注意力分布提供直观的输入与输出对应关系。\n",
    "5. 提高输出质量：生成更流畅、准确的序列。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderRNN(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_size, dropout = 0.1):\n",
    "        super(DecoderRNN, self).__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.hidden_size = hidden_size\n",
    "        self.gru = GRUCell(embedding_dim, hidden_size)\n",
    "\n",
    "        #add dropout\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        #add layer normalization\n",
    "        self.layer_norm = nn.LayerNorm(hidden_size)\n",
    "    \n",
    "        # self.h2o = nn.Linear(hidden_size, vocab_size) //replaced with self.out\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "        self.attention = BahdanauAttention(self.hidden_size)\n",
    "        self.attention_combine = nn.Linear(hidden_size + embedding_dim, embedding_dim)\n",
    "        \n",
    "        #multiple layers with activation\n",
    "        self.out = nn.Sequential(\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, vocab_size)\n",
    "        )\n",
    "    \n",
    "    def forward(self, input, hidden, encoder_outputs):\n",
    "        \"\"\"\n",
    "        input: N\n",
    "        hidden: N * H\n",
    "        \n",
    "        输出对于下一个时间片的预测output（大小为N * V）更新后的隐状态hidden（大小为N * H）\n",
    "        \"\"\"\n",
    "        embedding = self.embedding(input)\n",
    "        embedding = self.dropout(embedding)\n",
    "\n",
    "        context, attention_weights = self.attention(hidden, encoder_outputs)\n",
    "\n",
    "        #combine context and embedding\n",
    "        gru_input = torch.cat((embedding, context), dim=1)\n",
    "        gru_input = self.attention_combine(gru_input)\n",
    "        gru_input = F.relu(gru_input)\n",
    "\n",
    "        #GRU step\n",
    "        hidden = self.gru(gru_input, hidden)\n",
    "        hidden = self.layer_norm(hidden)\n",
    "\n",
    "        #output projection\n",
    "        output = self.out(hidden)\n",
    "        output = self.softmax(output)\n",
    "        return output, hidden, attention_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Label Smoothing Loss (标签平滑损失)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "标签平滑（Label Smoothing） 是一种在分类任务中常用的正则化技术，用于缓解模型过拟合和输出过于自信的问题。在训练时，模型通常以独热编码（One-Hot Encoding）的标签作为目标。\\\n",
    "\n",
    "例如，对于一个3类分类问题，如果正确类别是2，目标标签是 [0, 0, 1]. \n",
    "这种方式明确地告诉模型，某个类别的概率应该是100%，其他类别的概率是0%。\n",
    "\n",
    "然而，这种方式可能导致模型：\n",
    "\n",
    "过度自信（Overconfident）：模型在训练过程中可能倾向于输出接近1的概率值，而这在实际任务中并不总是最优的。\n",
    "过拟合（Overfitting）：模型可能对训练数据的标签分布过度适应，从而在测试数据上表现较差。\n",
    "\n",
    "所以根据大语言模型的推荐，我也实现了 label smoothing loss 来增加我的 Bleu score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add label smoothing loss\n",
    "class LabelSmoothingLoss(nn.Module):\n",
    "    def __init__(self, size, smoothing=0.1):\n",
    "        super(LabelSmoothingLoss, self).__init__()\n",
    "        self.criterion = nn.KLDivLoss(reduction='batchmean')\n",
    "        self.smoothing = smoothing\n",
    "        self.size = size\n",
    "        self.true_dist = None\n",
    "        \n",
    "    def forward(self, x, target):\n",
    "        assert x.size(1) == self.size\n",
    "        true_dist = x.data.clone()\n",
    "        true_dist.fill_(self.smoothing / (self.size - 1))\n",
    "        true_dist.scatter_(1, target.data.unsqueeze(1), 1.0 - self.smoothing)\n",
    "        return self.criterion(x, true_dist.detach())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Seq2Seq Model (全部合并)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, src_vocab, tgt_vocab, embedding_dim, hidden_size, max_len):\n",
    "        super(Seq2Seq, self).__init__()\n",
    "        self.src_vocab = src_vocab\n",
    "        self.tgt_vocab = tgt_vocab\n",
    "        self.hidden_size = hidden_size\n",
    "        self.encoder = EncoderRNN(len(src_vocab), embedding_dim, hidden_size)\n",
    "        self.decoder = DecoderRNN(len(tgt_vocab), embedding_dim, hidden_size)\n",
    "        self.max_len = max_len\n",
    "        \n",
    "    def init_hidden(self, batch_size):\n",
    "        \"\"\"\n",
    "        初始化编码器端隐状态为全0向量（大小为1 * H）\n",
    "        \"\"\"\n",
    "        device = next(self.parameters()).device\n",
    "        return torch.zeros(batch_size, self.hidden_size).to(device)\n",
    "    \n",
    "    def init_tgt_bos(self, batch_size):\n",
    "        \"\"\"\n",
    "        预测时，初始化解码器端输入为[BOS]（大小为batch_size）\n",
    "        \"\"\"\n",
    "        device = next(self.parameters()).device\n",
    "        return (torch.ones(batch_size)*self.tgt_vocab.index(\"[BOS]\")).long().to(device)\n",
    "    \n",
    "    def forward_encoder(self, src):\n",
    "        \"\"\"\n",
    "        src: N * L\n",
    "        编码器前向传播，输出最终隐状态hidden (N * H)和隐状态序列encoder_hiddens (N * L * H)\n",
    "        \"\"\"\n",
    "        Bs, Ls = src.size()\n",
    "        hidden = self.init_hidden(batch_size=Bs)\n",
    "        encoder_hiddens = []\n",
    "        # 编码器端每个时间片，取出输入单词的下标，与上一个时间片的隐状态一起送入encoder，得到更新后的隐状态，存入enocder_hiddens\n",
    "        for i in range(Ls):\n",
    "            input = src[:, i]\n",
    "            hidden = self.encoder(input, hidden)\n",
    "            encoder_hiddens.append(hidden)\n",
    "        encoder_hiddens = torch.stack(encoder_hiddens, dim=0)\n",
    "        return hidden, encoder_hiddens\n",
    "    \n",
    "    def forward_decoder(self, tgt, hidden, encoder_hiddens, tfr):\n",
    "        \"\"\"\n",
    "        tgt: N\n",
    "        hidden: N * H\n",
    "        \n",
    "        解码器前向传播，用于训练，使用teacher forcing，输出预测结果outputs，大小为N * L * V，其中V为目标语言词表大小\n",
    "        \"\"\"\n",
    "        Bs, Lt = tgt.size()\n",
    "        outputs = []\n",
    "        input = tgt[:, 0]\n",
    "\n",
    "        for i in range(Lt):\n",
    "            output, hidden, attention_weights = self.decoder(input, hidden, encoder_hiddens)\n",
    "            outputs.append(output)\n",
    "\n",
    "            use_teacher_forcing = random.random() < tfr\n",
    "            if use_teacher_forcing and i < Lt-1:\n",
    "                input = tgt[:, i+1]\n",
    "            else:\n",
    "                input = output.argmax(-1)\n",
    "        outputs = torch.stack(outputs, dim=1)\n",
    "        return outputs\n",
    "        \n",
    "    \n",
    "    def forward(self, src, tgt, tfr):\n",
    "        \"\"\"\n",
    "            src: 1 * Ls\n",
    "            tgt: 1 * Lt\n",
    "            \n",
    "            训练时的前向传播\n",
    "        \"\"\"\n",
    "        hidden, encoder_hiddens = self.forward_encoder(src)\n",
    "        outputs = self.forward_decoder(tgt, hidden, encoder_hiddens, tfr)\n",
    "        return outputs\n",
    "    \n",
    "    def predict(self, src):\n",
    "        \"\"\"\n",
    "            src: 1 * Ls\n",
    "            \n",
    "            用于预测，解码器端初始输入为[BOS]，之后每个位置的输入为上个时间片预测概率最大的单词\n",
    "            当解码长度超过self.max_len或预测出了[EOS]时解码终止\n",
    "            输出预测的单词编号序列，大小为1 * L，L为预测长度\n",
    "        \"\"\"\n",
    "        hidden, encoder_hiddens = self.forward_encoder(src)\n",
    "        input = self.init_tgt_bos(batch_size=src.shape[0])\n",
    "        preds = []\n",
    "        while len(preds) < self.max_len:\n",
    "            output, hidden, attention_weights = self.decoder(input, hidden, encoder_hiddens)\n",
    "            input = output.argmax(-1)\n",
    "            preds.append(input)\n",
    "            if input == self.tgt_vocab.index(\"[EOS]\"):\n",
    "                break\n",
    "        preds = torch.stack(preds, dim=-1)\n",
    "        return preds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 上载数据 （Data Loader）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "按照之前提供的，没什么改变来上载我们的数据，准备给我们的seq2seq 模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 构建Dataloader\n",
    "def collate(data_list):\n",
    "    src = torch.stack([torch.LongTensor(_[0]) for _ in data_list])\n",
    "    tgt = torch.stack([torch.LongTensor(_[1]) for _ in data_list])\n",
    "    return src, tgt\n",
    "\n",
    "def padding(inp_ids, max_len, pad_id):\n",
    "    max_len += 2    # include [BOS] and [EOS]\n",
    "    ids_ = np.ones(max_len, dtype=np.int32) * pad_id\n",
    "    max_len = min(len(inp_ids), max_len)\n",
    "    ids_[:max_len] = inp_ids\n",
    "    return ids_\n",
    "\n",
    "def create_dataloader(zh_sents, en_sents, max_len, batch_size, pad_id):\n",
    "    dataloaders = {}\n",
    "    for split in ['train', 'val', 'test']:\n",
    "        shuffle = True if split=='train' else False\n",
    "        datas = [(padding(zh_vocab.encode(zh, max_len), max_len, pad_id), padding(en_vocab.encode(en, max_len), max_len, pad_id)) for zh, en in zip(zh_sents[split], en_sents[split])]\n",
    "        dataloaders[split] = torch.utils.data.DataLoader(datas, batch_size=batch_size, shuffle=shuffle, collate_fn=collate)\n",
    "    return dataloaders['train'], dataloaders['val'], dataloaders['test']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 训练模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "开始训练我们的模型。 在此，大部分的编码跟之前的差不多，除了一个部分：teacher forcing 和 之前的 label smoothing loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Teacher Forcing 是一种在训练序列到序列模型（如翻译或生成任务）时常用的技术。其核心思想是：在每个解码步骤中，使用真实的目标词作为当前时间步的输入，而不是模型在上一时间步生成的预测词。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`TFR=max(0.4, 1.0 − current_epoch/max_epoch​)`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "意味着：\n",
    "在训练初期，TFR 接近 1，更多使用真实目标词（Teacher Forcing）；\n",
    "随着训练逐渐进行，TFR 减少至 0.4，模型逐渐更多依赖自己的预测进行训练\n",
    "\n",
    "这也是因为在文件里所说的，如果没有teacher forcing 的时候，在训练早期的输入可能无关，增加错误预测的传播，降低训练效率"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 训练模型\n",
    "def train_loop(model, optimizer, criterion, loader, device, current_epoch, max_epochs):\n",
    "    model.train()\n",
    "    epoch_loss = 0.0\n",
    "\n",
    "    #add label smoothing\n",
    "    smoothing = 0.1\n",
    "    criterion = LabelSmoothingLoss(size=model.decoder.vocab_size, smoothing=smoothing)\n",
    "\n",
    "    for src, tgt in tqdm(loader):\n",
    "        src = src.to(device)\n",
    "        tgt = tgt.to(device)\n",
    "\n",
    "        tfr = max(0.4, 1.0 - current_epoch/max_epochs)    # teacher forcing ratio\n",
    "        outputs = model(src, tgt, tfr)\n",
    "        loss = criterion(outputs[:,:-1,:].reshape(-1, outputs.shape[-1]), tgt[:,1:].reshape(-1))\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1)     # 裁剪梯度，将梯度范数裁剪为1，使训练更稳定\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "    epoch_loss /= len(loader)\n",
    "    return epoch_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 模型评价 （Evaluation）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "test_loop 的代码跟之前的一样，没有改变evaluation 的方式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_loop(model, loader, tgt_vocab, device):\n",
    "    model.eval()\n",
    "    bleu = BLEU(force=True)\n",
    "    hypotheses, references = [], []\n",
    "    for src, tgt in tqdm(loader):\n",
    "        B = len(src)\n",
    "        for _ in range(B):\n",
    "            _src = src[_].unsqueeze(0).to(device)     # 1 * L\n",
    "            with torch.no_grad():\n",
    "                outputs = model.predict(_src)         # 1 * L\n",
    "            \n",
    "            # 保留预测结果，使用词表vocab解码成文本，并删去BOS与EOS\n",
    "            ref = \" \".join(tgt_vocab.decode(tgt[_].tolist(), strip_bos_eos_pad=True))\n",
    "            hypo = \" \".join(tgt_vocab.decode(outputs[0].cpu().tolist(), strip_bos_eos_pad=True))\n",
    "            references.append(ref)    # 标准答案\n",
    "            hypotheses.append(hypo)   # 预测结果\n",
    "    \n",
    "    score = bleu.corpus_score(hypotheses, [references]).score      # 计算BLEU分数\n",
    "    return hypotheses, references, score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 主函数 （main function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练集 验证集 测试集大小分别为 26187 1000 1000\n",
      "中文词表大小为 14718\n",
      "英语词表大小为 11475\n",
      "Using device:  cuda\n",
      "GPU:  NVIDIA GeForce RTX 3050 Ti Laptop GPU\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 205/205 [00:18<00:00, 11.38it/s]\n",
      "100%|██████████| 8/8 [00:19<00:00,  2.50s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: loss = 3.1951938024381312, valid bleu = 0.7450135118611164\n",
      "Why are you [UNK] my son?\n",
      "Do you like to go to the good\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 205/205 [00:32<00:00,  6.24it/s]\n",
      "100%|██████████| 8/8 [00:18<00:00,  2.36s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: loss = 2.4383464161942645, valid bleu = 3.4159235725936137\n",
      "Why are you [UNK] my son?\n",
      "How much do you know me to me?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 205/205 [00:33<00:00,  6.05it/s]\n",
      "100%|██████████| 8/8 [00:19<00:00,  2.40s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: loss = 2.2229364912684373, valid bleu = 4.574956087086325\n",
      "Why are you [UNK] my son?\n",
      "Why did you please me to the\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 205/205 [00:35<00:00,  5.80it/s]\n",
      "100%|██████████| 8/8 [00:17<00:00,  2.25s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: loss = 2.1035038099056336, valid bleu = 5.642752907530907\n",
      "Why are you [UNK] my son?\n",
      "Why did you tell me to\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 205/205 [00:36<00:00,  5.62it/s]\n",
      "100%|██████████| 8/8 [00:19<00:00,  2.46s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: loss = 2.0026899651783268, valid bleu = 6.859306211078687\n",
      "Why are you [UNK] my son?\n",
      "Why do you know the\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 205/205 [00:36<00:00,  5.63it/s]\n",
      "100%|██████████| 8/8 [00:20<00:00,  2.61s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: loss = 1.9168356744254507, valid bleu = 7.908841070805155\n",
      "Why are you [UNK] my son?\n",
      "Why did you tell me the\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 205/205 [00:37<00:00,  5.53it/s]\n",
      "100%|██████████| 8/8 [00:21<00:00,  2.72s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6: loss = 1.8236620152868876, valid bleu = 9.267761823952052\n",
      "Why are you [UNK] my son?\n",
      "Why did you let me the\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 205/205 [00:37<00:00,  5.54it/s]\n",
      "100%|██████████| 8/8 [00:20<00:00,  2.59s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7: loss = 1.6883214944746436, valid bleu = 10.385686894551565\n",
      "Why are you [UNK] my son?\n",
      "Why did you call me to\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 205/205 [00:36<00:00,  5.64it/s]\n",
      "100%|██████████| 8/8 [00:22<00:00,  2.79s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8: loss = 1.5274026841652102, valid bleu = 11.486475430260418\n",
      "Why are you [UNK] my son?\n",
      "Why did you call me the\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 205/205 [00:36<00:00,  5.59it/s]\n",
      "100%|██████████| 8/8 [00:21<00:00,  2.70s/it]\n",
      "C:\\Users\\wenji\\AppData\\Local\\Temp\\ipykernel_27424\\2389720481.py:86: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"model_best.pt\"))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: loss = 1.3903224712464868, valid bleu = 13.644033303596743\n",
      "Why are you [UNK] my son?\n",
      "Why did you let me the the\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8/8 [00:26<00:00,  3.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test bleu = 14.456257891945436\n",
      "We should be safe here.\n",
      "We should be your right here.\n",
      "Training time: 9.07min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import argparse\n",
    "\n",
    "\n",
    "\n",
    "# 主函数\n",
    "if __name__ == '__main__':\n",
    "    sys.argv = [\n",
    "        'ipykernel_launcher.py',  \n",
    "        '--num_train', '-1',      \n",
    "        '--max_len', '10',\n",
    "        '--batch_size', '128',\n",
    "        '--optim', 'adam',\n",
    "        '--num_epoch', '10',\n",
    "        '--lr', '0.0005'\n",
    "    ]\n",
    "\n",
    "    parser = argparse.ArgumentParser()      \n",
    "    parser.add_argument('--num_train', default=-1, type=int, help=\"训练集大小，等于-1时将包含全部训练数据\")\n",
    "    parser.add_argument('--max_len', default=10, type=int, help=\"句子最大长度\")\n",
    "    parser.add_argument('--batch_size', default=128, type=int)\n",
    "    parser.add_argument('--optim', default='adam')\n",
    "    parser.add_argument('--num_epoch', default=10, type=int)\n",
    "    parser.add_argument('--lr', default=0.0005, type=float)\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    zh_sents, en_sents = load_data(args.num_train)\n",
    "\n",
    "    zh_vocab = Vocab()\n",
    "    en_vocab = Vocab()\n",
    "    for zh, en in zip(zh_sents['train'], en_sents['train']):\n",
    "        zh_vocab.add_sent(zh)\n",
    "        en_vocab.add_sent(en)\n",
    "    print(\"中文词表大小为\", len(zh_vocab))\n",
    "    print(\"英语词表大小为\", len(en_vocab))\n",
    "\n",
    "    trainloader, validloader, testloader = create_dataloader(zh_sents, en_sents, args.max_len, args.batch_size, pad_id=zh_vocab.word2idx['[PAD]'])\n",
    "\n",
    "    torch.manual_seed(1)\n",
    "    #Use GPU for training\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(\"Using device: \", device)\n",
    "    if torch.cuda.is_available():\n",
    "        print(\"GPU: \", torch.cuda.get_device_name(0))\n",
    "\n",
    "    #initialise Model\n",
    "    model = Seq2Seq(zh_vocab, en_vocab, embedding_dim=256, hidden_size=256, max_len=args.max_len)\n",
    "    model.to(device)\n",
    "    if args.optim=='sgd':\n",
    "        optimizer = torch.optim.SGD(model.parameters(), lr=args.lr)\n",
    "    elif args.optim=='adam':\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr = args.lr) #introduce more parameters for Adam\n",
    "    weights = torch.ones(len(en_vocab)).to(device)\n",
    "    weights[en_vocab.word2idx['[PAD]']] = 0 # set the loss of [PAD] to zero\n",
    "    criterion = nn.NLLLoss(weight=weights)\n",
    "\n",
    "    # #introduce scheduler\n",
    "    # scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    #     optimizer,\n",
    "    #     mode='max',\n",
    "    #     factor=0.5,\n",
    "    #     patience=2,\n",
    "    #     verbose=True\n",
    "    # )\n",
    "\n",
    "    # 训练\n",
    "    start_time = time.time()\n",
    "    best_score = 0.0\n",
    "    best_epoch = 0\n",
    "    \n",
    "    for epoch in range(args.num_epoch):\n",
    "        loss = train_loop(model, optimizer, criterion, trainloader, device, epoch, args.num_epoch)\n",
    "        hypotheses, references, bleu_score = test_loop(model, validloader, en_vocab, device)\n",
    "        # scheduler.step(bleu_score)\n",
    "        # 保存验证集上bleu最高的checkpoint\n",
    "        if bleu_score > best_score:\n",
    "            torch.save(model.state_dict(), \"model_best.pt\")\n",
    "            best_score = bleu_score\n",
    "            best_epoch = epoch\n",
    "        print(f\"Epoch {epoch}: loss = {loss}, valid bleu = {bleu_score}\")\n",
    "        print(references[0])\n",
    "        print(hypotheses[0])\n",
    "    end_time = time.time()\n",
    "\n",
    "    #测试\n",
    "    model.load_state_dict(torch.load(\"model_best.pt\"))\n",
    "    hypotheses, references, bleu_score = test_loop(model, testloader, en_vocab, device)\n",
    "    print(f\"Test bleu = {bleu_score}\")\n",
    "    print(references[0])\n",
    "    print(hypotheses[0])\n",
    "    print(f\"Training time: {round((end_time - start_time)/60, 2)}min\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "最终，我们把所有实现的都放在主函数下，并且训练而得到一个很不错的bleu score （~14）\n",
    "\n",
    "在这里我也尝试了使用optimizer with weight decay 和 scheduler 可是这些导致我的模型训练的结果不符合要求（bleu score 一下子降低到0.2\n",
    "\n",
    "一下可能是bleu score 降低的原因：\n",
    "破坏模型的记忆能力\n",
    "过于频繁地调整学习率\n",
    "学习率降低过早\n",
    "\n",
    "所以最终还是去掉了区别"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 实验结果与分析"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 模型性能\n",
    "\n",
    "1. BLEU分数：模型在测试集上取得了 14.46 的 BLEU 分数，表明模型在句子翻译或生成任务中具备一定的语言建模能力。\n",
    "BLEU分数在翻译任务中是一种常用的评估指标，分数范围在 0 到 100 之间。我的模型取得的分数显示翻译效果已达到初步可用的水平，但仍有改进空间。\n",
    "\n",
    "2. 根据readme.md 的参考BLEU，Attention RNN Decoder + GRU 的 BLEU 分数在 13左右，表明翻译结果质量尚可，但在流畅性和准确性上可能还存在一定不足。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 缺点：\n",
    "\n",
    "1. 未识别词（[UNK]）的出现反映词表覆盖不足的问题。\n",
    "\n",
    "2. 句子重复现象（\"the the\"）和语法不连贯的问题显示模型在复杂句子上的生成能力仍需提升。\n",
    "\n",
    "3. BLEU 分数虽然不错，但距离高质量翻译任务的水平仍有较大差距。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 改进方向\n",
    "\n",
    "1. 数据增强：\n",
    "增加训练数据的多样性，尤其是包含更丰富词汇和复杂句式的语料。\n",
    "\n",
    "2. 优化模型结构：\n",
    "增加词表大小或采用子词分词（如 BPE）减少 [UNK] 的出现。\n",
    "在解码器中引入更先进的机制（如 Transformer）以提升模型的语义和语法理解能力。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
